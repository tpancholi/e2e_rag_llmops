{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Simple RAG demo with all the modules\n",
    "- Langchain document loader\n",
    "- custom text splitter\n",
    "- FAISS vector db\n",
    "- Hugginface embedding\n",
    "- LLM model\n",
    "- RAG chain using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "# load env variables\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "try:  # inside a script\n",
    "    BASE_DIR = Path(__file__).resolve().parent.parent\n",
    "except NameError:  # inside a notebook\n",
    "    BASE_DIR = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input_file = BASE_DIR / \"data\" / \"sample_input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data Ingestion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchai load the document for further processing\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(sample_input_file, encoding=\"utf-8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us split the document into manageable chunks using a custom text splitter\n",
    "# based on research on link - https://research.trychroma.com/evaluating-chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200, chunk_overlap=0, separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the document into manageable chunks using text splitter\n",
    "text_chunks = text_splitter.split_documents(documents=documents)\n",
    "text_chunks[0]  # verify the structure of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of chunks generated from the document\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary library and initializing an embedding model from huggingface\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"google/embeddinggemma-300m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding function\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs={\"device\": \"cpu\"}, encode_kwargs={\"normalize_embeddings\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### in case of custom embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(documents=text_chunks, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving vector db into local for reuse\n",
    "vector_db_local = BASE_DIR / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.save_local(folder_path=vector_db_local, index_name=\"vector_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Retrival using cosine based similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main components of a comprehensive Agentic AI testing system?\"\n",
    "relevant_chunks = vector_db.similarity_search(query, k=5)\n",
    "for idx, doc in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {idx + 1}: {doc.page_content}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Response generation with the help of relevant chunks from vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI assistant that helps users with their questions.\n",
    "You are given a question and a set of relevant chunks from vector db.\n",
    "You need to generate a response that is relevant to the question and the chunks.\n",
    "If you do not know the answer, just say that you do not know.\n",
    "Use maximum ten sentences and keep the answer concise.\n",
    "Question: {question}\n",
    "Relevant chunks: {chunks}\n",
    "Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the prompt instance\n",
    "prompt = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output parser to format response\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup llm model\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm_model = ChatOpenAI(model=\"gpt-5-nano\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vector db as retriever\n",
    "retriever = vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating chain using LCEL\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel({\"question\": RunnablePassthrough(), \"chunks\": RunnablePassthrough() | retriever})\n",
    "    | prompt\n",
    "    | llm_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
